{
  "metadata": {
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    },
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    }
  },
  "nbformat_minor": 4,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": "import csv\nfrom sklearn.metrics import f1_score, confusion_matrix\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom sklearn.feature_extraction.text import CountVectorizer\nimport matplotlib.pyplot as plt\nimport numpy as np",
      "metadata": {
        "trusted": true
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "questions = []\nhuman_answers = []\nbot_answers = []",
      "metadata": {
        "trusted": true
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "with open('accuracy.csv') as f:\n    reader = csv.reader(f)\n    next(reader) # discard csv headers\n    for row in reader:\n        bot_answer, human_answer, question = row.pop(), row.pop(), row.pop()\n        bot_answers.append(bot_answer)\n        human_answers.append(human_answer)\n        questions.append(question)",
      "metadata": {
        "trusted": true
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# create a CountVectorizer object\nvectorizer = CountVectorizer()\ncosines = []\n# fit the vectorizer on the sentences\nfor i in range(len(questions)):\n    vectorizer.fit_transform([human_answers[i], bot_answers[i]])\n    # transform the sentences into vectors\n    vector1 = vectorizer.transform([human_answers[i]]).toarray()\n    vector2 = vectorizer.transform([bot_answers[i]]).toarray()\n    # calculate cosine similarity between the two vectors\n    cosine_sim = cosine_similarity(vector1, vector2)\n    cosines.append(cosine_sim.flatten()[0])\n    \nprint(cosines)",
      "metadata": {
        "trusted": true
      },
      "execution_count": 13,
      "outputs": [
        {
          "name": "stdout",
          "text": "[0.9655402033694289, 0.9999999999999999, 0.7915594835766295, 0.9545454545454546, 0.6607934284572003, 0.5956098817361238, 0.6546536707079771, 0.7071067811865476, 0.618688224889746, 0.8261527758779903]\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": "# compute predicted labels\n# if the value of cosine similarity is bigger than 0.7 consider as correct ('1')\nthreshold = 0.7\npredicted_labels = [1 if value > threshold else 0 for value in cosines]\n\n# Print the predicted values\nprint(predicted_labels)",
      "metadata": {
        "trusted": true
      },
      "execution_count": 14,
      "outputs": [
        {
          "name": "stdout",
          "text": "[1, 1, 1, 1, 0, 0, 0, 1, 0, 1]\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": "# true labels are the control group of correct answers, all values in this list are correct ('1')\ntrue_labels = [1 for i in range(len(predicted_labels))]",
      "metadata": {
        "trusted": true
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "predicted_classes = np.where(np.array(predicted_labels) >= threshold, 1, 0)",
      "metadata": {
        "trusted": true
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# Compute confusion matrix\nconfusion = confusion_matrix(np.array(true_labels), predicted_classes)\n# true negatives (TN) false positives (FP) false negatives (FN) true positives (TP)\ntn, fp, fn, tp = confusion.ravel()",
      "metadata": {
        "trusted": true
      },
      "execution_count": 20,
      "outputs": [
        {
          "name": "stdout",
          "text": "[0 0 4 6]\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": "# calculate f1_score given the true and predicted labels\nf1_simple = f1_score(true_labels, predicted_labels)\nf1 = f1_score(true_labels, predicted_classes)\nprint(\"F1 score simple:\", f1_simple)\nprint(\"F1 score:\", f1)",
      "metadata": {
        "trusted": true
      },
      "execution_count": 19,
      "outputs": [
        {
          "name": "stdout",
          "text": "F1 score simple: 0.7499999999999999\nF1 score: 0.7499999999999999\n",
          "output_type": "stream"
        }
      ]
    }
  ]
}